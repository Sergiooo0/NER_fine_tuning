---
library_name: transformers
base_model: dccuchile/bert-base-spanish-wwm-cased
tags:
- generated_from_trainer
metrics:
- precision
- recall
- f1
- accuracy
model-index:
- name: bert-base-spanish-wwm-cased-ner
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# bert-base-spanish-wwm-cased-ner

This model is a fine-tuned version of [dccuchile/bert-base-spanish-wwm-cased](https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.0974
- Precision: 0.8664
- Recall: 0.8784
- F1: 0.8724
- Accuracy: 0.9807

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 32
- eval_batch_size: 8
- seed: 42
- optimizer: Use OptimizerNames.ADAMW_TORCH with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: linear
- num_epochs: 20.0

### Training results

| Training Loss | Epoch   | Step | Validation Loss | Precision | Recall | F1     | Accuracy |
|:-------------:|:-------:|:----:|:---------------:|:---------:|:------:|:------:|:--------:|
| 0.1848        | 0.3831  | 100  | 0.1168          | 0.7770    | 0.8227 | 0.7992 | 0.9715   |
| 0.0688        | 0.7663  | 200  | 0.0903          | 0.8460    | 0.8448 | 0.8454 | 0.9769   |
| 0.048         | 1.1494  | 300  | 0.0943          | 0.8374    | 0.8557 | 0.8465 | 0.9776   |
| 0.0363        | 1.5326  | 400  | 0.0826          | 0.8508    | 0.8718 | 0.8612 | 0.9801   |
| 0.0373        | 1.9157  | 500  | 0.0906          | 0.8375    | 0.8455 | 0.8415 | 0.9776   |
| 0.0243        | 2.2989  | 600  | 0.0940          | 0.8523    | 0.8632 | 0.8577 | 0.9788   |
| 0.0253        | 2.6820  | 700  | 0.1008          | 0.8364    | 0.8541 | 0.8452 | 0.9781   |
| 0.0204        | 3.0651  | 800  | 0.0974          | 0.8664    | 0.8784 | 0.8724 | 0.9807   |
| 0.0138        | 3.4483  | 900  | 0.1045          | 0.8322    | 0.8623 | 0.8470 | 0.9778   |
| 0.0154        | 3.8314  | 1000 | 0.1020          | 0.8628    | 0.8763 | 0.8695 | 0.9804   |
| 0.0139        | 4.2146  | 1100 | 0.1107          | 0.8547    | 0.8681 | 0.8614 | 0.9786   |
| 0.0114        | 4.5977  | 1200 | 0.1068          | 0.8579    | 0.8639 | 0.8609 | 0.9795   |
| 0.0097        | 4.9808  | 1300 | 0.1167          | 0.8560    | 0.8688 | 0.8624 | 0.9794   |
| 0.0079        | 5.3640  | 1400 | 0.1203          | 0.8462    | 0.8630 | 0.8545 | 0.9784   |
| 0.0069        | 5.7471  | 1500 | 0.1177          | 0.8511    | 0.8606 | 0.8559 | 0.9785   |
| 0.007         | 6.1303  | 1600 | 0.1348          | 0.8445    | 0.8529 | 0.8487 | 0.9780   |
| 0.0056        | 6.5134  | 1700 | 0.1215          | 0.8519    | 0.8592 | 0.8556 | 0.9788   |
| 0.0057        | 6.8966  | 1800 | 0.1212          | 0.8619    | 0.8739 | 0.8679 | 0.9800   |
| 0.0047        | 7.2797  | 1900 | 0.1290          | 0.8604    | 0.8702 | 0.8653 | 0.9801   |
| 0.0037        | 7.6628  | 2000 | 0.1461          | 0.8448    | 0.8578 | 0.8513 | 0.9784   |
| 0.0048        | 8.0460  | 2100 | 0.1371          | 0.8632    | 0.8735 | 0.8683 | 0.9798   |
| 0.0034        | 8.4291  | 2200 | 0.1419          | 0.8483    | 0.8655 | 0.8568 | 0.9788   |
| 0.004         | 8.8123  | 2300 | 0.1397          | 0.8655    | 0.8758 | 0.8706 | 0.9801   |
| 0.0029        | 9.1954  | 2400 | 0.1518          | 0.8506    | 0.8597 | 0.8551 | 0.9784   |
| 0.0034        | 9.5785  | 2500 | 0.1542          | 0.8602    | 0.8707 | 0.8654 | 0.9795   |
| 0.0029        | 9.9617  | 2600 | 0.1465          | 0.8596    | 0.8693 | 0.8644 | 0.9795   |
| 0.0032        | 10.3448 | 2700 | 0.1542          | 0.8495    | 0.8574 | 0.8534 | 0.9790   |
| 0.0031        | 10.7280 | 2800 | 0.1392          | 0.8665    | 0.8725 | 0.8695 | 0.9800   |
| 0.0025        | 11.1111 | 2900 | 0.1378          | 0.8652    | 0.8735 | 0.8693 | 0.9801   |
| 0.0016        | 11.4943 | 3000 | 0.1585          | 0.8535    | 0.8690 | 0.8612 | 0.9794   |
| 0.0026        | 11.8774 | 3100 | 0.1565          | 0.8472    | 0.8658 | 0.8564 | 0.9793   |
| 0.0026        | 12.2605 | 3200 | 0.1533          | 0.8576    | 0.8700 | 0.8637 | 0.9796   |
| 0.0021        | 12.6437 | 3300 | 0.1588          | 0.8571    | 0.8653 | 0.8612 | 0.9793   |
| 0.0024        | 13.0268 | 3400 | 0.1470          | 0.8613    | 0.8711 | 0.8662 | 0.9804   |
| 0.0018        | 13.4100 | 3500 | 0.1584          | 0.8567    | 0.8683 | 0.8625 | 0.9795   |
| 0.002         | 13.7931 | 3600 | 0.1622          | 0.8521    | 0.8700 | 0.8609 | 0.9792   |
| 0.0018        | 14.1762 | 3700 | 0.1624          | 0.8550    | 0.8641 | 0.8595 | 0.9792   |
| 0.0014        | 14.5594 | 3800 | 0.1677          | 0.8617    | 0.8611 | 0.8614 | 0.9791   |
| 0.0022        | 14.9425 | 3900 | 0.1622          | 0.8525    | 0.8634 | 0.8579 | 0.9789   |
| 0.0011        | 15.3257 | 4000 | 0.1666          | 0.8549    | 0.8625 | 0.8587 | 0.9789   |
| 0.0014        | 15.7088 | 4100 | 0.1640          | 0.8600    | 0.8702 | 0.8651 | 0.9795   |
| 0.0012        | 16.0920 | 4200 | 0.1639          | 0.8598    | 0.8730 | 0.8663 | 0.9798   |
| 0.0011        | 16.4751 | 4300 | 0.1645          | 0.8634    | 0.8746 | 0.8690 | 0.9801   |
| 0.0013        | 16.8582 | 4400 | 0.1676          | 0.8577    | 0.8681 | 0.8629 | 0.9793   |
| 0.0017        | 17.2414 | 4500 | 0.1741          | 0.8554    | 0.8616 | 0.8585 | 0.9788   |
| 0.0014        | 17.6245 | 4600 | 0.1683          | 0.8578    | 0.8690 | 0.8634 | 0.9797   |
| 0.001         | 18.0077 | 4700 | 0.1718          | 0.8556    | 0.8655 | 0.8605 | 0.9792   |
| 0.0009        | 18.3908 | 4800 | 0.1675          | 0.8595    | 0.8681 | 0.8638 | 0.9795   |
| 0.0009        | 18.7739 | 4900 | 0.1702          | 0.8597    | 0.8683 | 0.8640 | 0.9796   |
| 0.0011        | 19.1571 | 5000 | 0.1706          | 0.8616    | 0.8704 | 0.8660 | 0.9798   |
| 0.0008        | 19.5402 | 5100 | 0.1706          | 0.8590    | 0.8686 | 0.8637 | 0.9797   |
| 0.0009        | 19.9234 | 5200 | 0.1704          | 0.8598    | 0.8690 | 0.8644 | 0.9797   |


### Framework versions

- Transformers 4.51.3
- Pytorch 2.6.0+cu124
- Datasets 3.6.0
- Tokenizers 0.21.1
